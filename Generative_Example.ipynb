{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generating/initializing random topics and words\n",
    "\n",
    "- Collection of basic blogs at bottom https://devo-evo.lab.asu.edu/methods/?q=node/42\n",
    "- The original Edwin Chen github repo on Sarah Palin: https://github.com/echen/sarah-palin-lda "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"lda-template.png\" width=\"350\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Examples of how to use the above functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Make some random assignments\n",
    "\n",
    "Topic distribution is $\\theta \\sim Dir( \\alpha )$, so we'd like $P(\\theta|\\alpha)$. These are sampled once per document; therefore, there are $M$ of them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The probability distribution of the topics are: [ 0.74118048  0.25881952]\n"
     ]
    }
   ],
   "source": [
    "alphas = [5,5]\n",
    "theta = np.random.dirichlet( alphas )\n",
    "print \"The probability distribution of the topics are: \"+str(theta)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Given a topic distribution as specified by $P(\\theta|\\alpha)$, we'd like to take $N$ samples  from the distribution specified by $\\theta$, each of which is called $z_n$. So,\n",
    "\n",
    "$z_n \\sim Multi( \\theta )$\n",
    "\n",
    "Formally, this is $P(z_n | \\theta) = \\theta_2^{z_n^{(2)}} \\cdots \\theta_k^{z_n^{(k)}}$ is a multinomial distribution, with parameter $\\theta$. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 1, 0, 1, 1, 1, 0, 1, 1, 1])"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "N = 10\n",
    "z_n = np.random.multinomial( 1, theta, size=N )\n",
    "z_n.argmax(axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So, we have two random variables:\n",
    "\n",
    "1. $\\theta \\sim Dir(\\alpha)$\n",
    "2. $z_n \\sim Multi(\\theta)$. (It's actually the Discrete RV, but don't worry about it)\n",
    "\n",
    "We now know $P( z_n | \\theta )$, but, what is $P( \\theta | z_n )$? So, if I observe $N$ topic assignments, what's the conditional distribution given those topic assignments? This is what we'd actually like to solve. As it turns out, because of something called *conjugacy* between the Dirichlet Process and the Multinomial, the posterior probability of the $\\theta$ parameter is also a Dirichlet process, and it can be specified as:\n",
    "\n",
    "$\\theta |z_n \\sim Dir( \\alpha + n( z_{1:N} ) )$\n",
    "\n",
    "where $n(z_{1:N})$ is the number of times we see a word in that document. So, the more words we see, and the more examples we see, the peakier the distribution gets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "theta_given_z = "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Given the topic $z_n$ and an overall word distribution as specified by $\\beta$, the word distribution is a multinomial distribution with parameter $\\beta$. Here, the parameter $\\beta$ is a matrix of size $k \\times V$, since $z_n \\in [1, k]$, i.e. $k$ topics, and there are $V$ words.\n",
    "\n",
    "The probability $P( w | z_n, \\beta_{1:K} )$ is a proper distribution whose columns and rows both sum to one."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "w_t_n = np.random.multinomial( V, betas, size=N )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With the given topic distribution $\\theta = [\\theta_1, \\theta_2, \\theta_3, \\cdots, \\theta_M$], the probability of that the $n^{th}$ word is from topic $k$ is $P( z_n = k | \\theta )$, which is simply $\\theta_k$. That is to say, the probability that $z_n^{(k)} = 1$, or $P(z_n^{(k)}=1 | \\theta, \\alpha)$ is simply the parameter $\\theta_k$. \n",
    "\n",
    "Then, we can write $P( z_n, \\theta | \\alpha ) = P( z_n | \\alpha) P(\\theta | \\alpha)$. If we are looking over $N$ words and each of them the draws are independent, then we have $P( \\theta, z | \\alpha ) = \\prod_n P( z_n | \\alpha ) P(\\theta | \\alpha ) = P(\\theta | \\alpha) \\prod_n P( z_n | \\theta )$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
